# Micrograd Implementation

This project is my implementation of the micrograd algorithm, following along with Andrej Karpathy's video tutorial. Through this implementation, I've gained a deeper understanding of how backpropagation works in neural networks.

## What I Learned

- The fundamental principles of automatic differentiation
- How gradients flow backward through a computational graph
- The implementation of basic neural network operations
- The mechanics of backpropagation in practice

## Resources

- [Andrej Karpathy's Micrograd Video](https://www.youtube.com/watch?v=VMj-3S1tku0)

## Implementation Details

The implementation includes:
- A `Value` class that represents a node in the computational graph
- Basic operations (addition, multiplication, etc.) with automatic gradient computation

This project serves as a practical exercise in understanding the core concepts of neural network training and gradient-based optimization.
